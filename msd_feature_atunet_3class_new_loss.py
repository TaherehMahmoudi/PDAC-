# -*- coding: utf-8 -*-
"""MSD_Feature_AtUnet_3class_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZQbc4oje51YJqev808vQUBxcLAyGFXYY
"""



import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
import torch.optim as optim
from torch.optim import lr_scheduler
import time
import copy
from torch.autograd import  Variable
from collections import defaultdict
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, datasets, models
import numpy as np
import random
from skimage.feature import local_binary_pattern
import os
import glob
import json
from scipy.ndimage.morphology import distance_transform_edt as edt
from scipy.ndimage import convolve

class conv_block(nn.Module):
    def __init__(self,ch_in,ch_out):
        super(conv_block,self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),
            nn.BatchNorm2d(ch_out),
            nn.ReLU(inplace=True),
            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),
            nn.BatchNorm2d(ch_out),
            nn.ReLU(inplace=True)
        )


    def forward(self,x):
        x = self.conv(x)
        return x

class up_conv(nn.Module):
    def __init__(self,ch_in,ch_out):
        super(up_conv,self).__init__()
        self.up = nn.Sequential(
            #nn.Upsample(scale_factor=2, mode='nearest'),
            
            #nn.Upsample(size=(h,w), mode='nearest'),
            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),
		    nn.BatchNorm2d(ch_out),
			nn.ReLU(inplace=True)
        )

    def forward(self,x,h,w):
        x=nn.Upsample(size=(h,w), mode='nearest')(x)
        x = self.up(x)
        return x


class single_conv(nn.Module):
    def __init__(self,ch_in,ch_out):
        super(single_conv,self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),
            nn.BatchNorm2d(ch_out),
            nn.ReLU(inplace=True)
        )

    def forward(self,x):
        x = self.conv(x)
        return x

class Attention_block(nn.Module):
    def __init__(self,F_g,F_l,F_int):
        super(Attention_block,self).__init__()
        self.W_g = nn.Sequential(
            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),
            nn.BatchNorm2d(F_int)
            )
        
        self.W_x = nn.Sequential(
            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),
            nn.BatchNorm2d(F_int)
        )

        self.psi = nn.Sequential(
            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )
        
        self.relu = nn.ReLU(inplace=True)
        
    def forward(self,g,x):
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        psi = self.relu(g1+x1)
        psi = self.psi(psi)
        return x*psi

def flatten(tensor):
    """Flattens a given tensor such that the channel axis is first.
    The shapes are transformed as follows:
       (N, C, D, H, W) -> (C, N * D * H * W)
    """
    C = tensor.size(1)
    # new axis order
    axis_order = (1, 0) + tuple(range(2, tensor.dim()))
    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)
    transposed = tensor.permute(axis_order).contiguous()
    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)
    return transposed.view(C, -1)


def one_hot(label, n_classes, requires_grad=True):
    """Return One Hot Label"""
    divce = label.device
    print(label.shape)
    one_hot_label = torch.eye(
        n_classes, device=device, requires_grad=requires_grad)[label.long()]
    one_hot_label = one_hot_label.transpose(1, 3).transpose(2, 3)
    print(one_hot_label.shape)
    return one_hot_label

################################################ tversky loss #############
def sum_tensor(inp, axes, keepdim=False):
    # copy from: https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/utilities/tensor_utilities.py
    axes = np.unique(axes).astype(int)
    if keepdim:
        for ax in axes:
            inp = inp.sum(int(ax), keepdim=True)
    else:
        for ax in sorted(axes, reverse=True):
            inp = inp.sum(int(ax))
    return inp


def get_tp_fp_fn(net_output, gt, axes=None, mask=None, square=False):
    """
    net_output must be (b, c, x, y(, z)))
    gt must be a label map (shape (b, 1, x, y(, z)) OR shape (b, x, y(, z))) or one hot encoding (b, c, x, y(, z))
    if mask is provided it must have shape (b, 1, x, y(, z)))
    :param net_output:
    :param gt:
    :param axes:
    :param mask: mask must be 1 for valid pixels and 0 for invalid pixels
    :param square: if True then fp, tp and fn will be squared before summation
    :return:
    """
    if axes is None:
        axes = tuple(range(2, len(net_output.size())))

    shp_x = net_output.shape
    shp_y = gt.shape

    with torch.no_grad():
        if len(shp_x) != len(shp_y):
            gt = gt.view((shp_y[0], 1, *shp_y[1:]))

        if all([i == j for i, j in zip(net_output.shape, gt.shape)]):
            # if this is the case then gt is probably already a one hot encoding
            y_onehot = gt
        else:
            gt = gt.long()
            y_onehot = torch.zeros(shp_x)
            if net_output.device.type == "cuda":
                y_onehot = y_onehot.cuda(net_output.device.index)
            y_onehot.scatter_(1, gt, 1)

    tp = net_output * y_onehot
    fp = net_output * (1 - y_onehot)
    fn = (1 - net_output) * y_onehot

    if mask is not None:
        tp = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(tp, dim=1)), dim=1)
        fp = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(fp, dim=1)), dim=1)
        fn = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(fn, dim=1)), dim=1)

    if square:
        tp = tp ** 2
        fp = fp ** 2
        fn = fn ** 2

    tp = sum_tensor(tp, axes, keepdim=False)
    fp = sum_tensor(fp, axes, keepdim=False)
    fn = sum_tensor(fn, axes, keepdim=False)

    return tp, fp, fn


class TverskyLoss(nn.Module):
    def __init__(self, apply_nonlin=None, batch_dice=False, do_bg=True, smooth=1.,
                 square=False):
        """
        paper: https://arxiv.org/pdf/1706.05721.pdf
        """
        super(TverskyLoss, self).__init__()

        self.square = square
        self.do_bg = do_bg
        self.batch_dice = batch_dice
        self.apply_nonlin = apply_nonlin
        self.smooth = smooth
        self.alpha = 0.5
        self.beta = 0.5

    def forward(self, x, y, loss_mask=None):
        shp_x = x.shape

        if self.batch_dice:
            axes = [0] + list(range(2, len(shp_x)))
        else:
            axes = list(range(2, len(shp_x)))

        if self.apply_nonlin is not None:
            x = self.apply_nonlin(x)

        tp, fp, fn = get_tp_fp_fn(x, y, axes, loss_mask, self.square)


        tversky = (tp + self.smooth) / (tp + self.alpha*fp + self.beta*fn + self.smooth)

        if not self.do_bg:
            if self.batch_dice:
                tversky = tversky[1:]
            else:
                tversky = tversky[:, 1:]
        tversky = tversky.mean()

        return -tversky

class FocalTversky_loss(nn.Module):
    """
    paper: https://arxiv.org/pdf/1810.07842.pdf
    author code: https://github.com/nabsabraham/focal-tversky-unet/blob/347d39117c24540400dfe80d106d2fb06d2b99e1/losses.py#L65
    """
    def __init__(self, tversky_kwargs, gamma=0.75):
        super(FocalTversky_loss, self).__init__()
        self.gamma = gamma
        self.tversky = TverskyLoss(**tversky_kwargs)

    def forward(self, net_output, target):
        tversky_loss = 1 + self.tversky(net_output, target) # = 1-tversky(net_output, target)
        focal_tversky = torch.pow(tversky_loss, self.gamma)
        return focal_tversky


########################################## boundary loss ####################
class BoundaryLoss(nn.Module):
    """Boundary Loss proposed in:
    Alexey Bokhovkin et al., Boundary Loss for Remote Sensing Imagery Semantic Segmentation
    https://arxiv.org/abs/1905.07852
    """

    def __init__(self, theta0=3, theta=5):
        super().__init__()

        self.theta0 = theta0
        self.theta = theta

    def forward(self, pred, gt):
        """
        Input:
            - pred: the output from model (before softmax)
                    shape (N, C, H, W)
            - gt: ground truth map
                    shape (N, H, w)
        Return:
            - boundary loss, averaged over mini-bathc
        """

        n, c, _, _ = pred.shape
        
        # softmax so that predicted map can be distributed in [0, 1]
        pred = torch.softmax(pred, dim=1)

        # one-hot vector of ground truth
        one_hot_gt = gt

        # boundary map
        gt_b = F.max_pool2d(
            1 - one_hot_gt, kernel_size=self.theta0, stride=1, padding=(self.theta0 - 1) // 2)
        gt_b -= 1 - one_hot_gt
        pred_b = F.max_pool2d(
            1 - pred, kernel_size=self.theta0, stride=1, padding=(self.theta0 - 1) // 2)
        pred_b -= 1 - pred

        # extended boundary map
        gt_b_ext = F.max_pool2d(
            gt_b, kernel_size=self.theta, stride=1, padding=(self.theta - 1) // 2)
        pred_b_ext = F.max_pool2d(
            pred_b, kernel_size=self.theta, stride=1, padding=(self.theta - 1) // 2)

        # reshape
        gt_b = gt_b.view(n, c, -1)
        pred_b = pred_b.view(n, c, -1)
        gt_b_ext = gt_b_ext.view(n, c, -1)
        pred_b_ext = pred_b_ext.view(n, c, -1)

        # Precision, Recall
        P = torch.sum(pred_b * gt_b_ext, dim=2) / (torch.sum(pred_b, dim=2) + 1e-7)
        R = torch.sum(pred_b_ext * gt_b, dim=2) / (torch.sum(gt_b, dim=2) + 1e-7)

        # Boundary F1 Score
        BF1 = 2 * P * R / (P + R + 1e-7)

        # summing BF1 Score for each class and average over mini-batch
        loss = torch.mean(1 - BF1)

        return loss
####################################################################

class HausdorffDTLoss(nn.Module):
    """Binary Hausdorff loss based on distance transform"""
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    def __init__(self, alpha=2.0, **kwargs):
        super(HausdorffDTLoss, self).__init__()
        self.alpha = alpha

    @torch.no_grad()
    def distance_field(self, img: np.ndarray) -> np.ndarray:
        field = np.zeros_like(img)

        for batch in range(len(img)):
            fg_mask = img[batch] > 0.5

            if fg_mask.any():
                bg_mask = ~fg_mask

                fg_dist = edt(fg_mask)
                bg_dist = edt(bg_mask)

                field[batch] = fg_dist + bg_dist

        return field

    def forward(
        self, pred, target, debug=False
    ) -> torch.Tensor:
        """
        Uses one binary channel: 1 - fg, 0 - bg
        pred: (b, 1, x, y, z) or (b, 1, x, y)
        target: (b, 1, x, y, z) or (b, 1, x, y)
        """
        assert pred.dim() == 4 or pred.dim() == 5, "Only 2D and 3D supported"
        assert (
            pred.dim() == target.dim()
        ), "Prediction and target need to be of same dimension"

        # pred = torch.sigmoid(pred)

        pred_dt = torch.from_numpy(self.distance_field(pred.cpu().detach().numpy())).float()
        target_dt = torch.from_numpy(self.distance_field(target.cpu().detach().numpy())).float()

        pred_error = (pred - target) ** 2
        distance = pred_dt ** self.alpha + target_dt ** self.alpha

        dt_field = pred_error * distance.to(device)
        loss = dt_field.mean()

        if debug:
            return (
                loss.cpu().numpy(),
                (
                    dt_field.cpu().numpy()[0, 0],
                    pred_error.cpu().numpy()[0, 0],
                    distance.cpu().numpy()[0, 0],
                    pred_dt.cpu().numpy()[0, 0],
                    target_dt.cpu().numpy()[0, 0],
                ),
            )

        else:
            return loss



class GDiceLossV2(nn.Module):
    def __init__(self, apply_nonlin=None, smooth=1e-5):
        """
        Generalized Dice;
        Copy from: https://github.com/wolny/pytorch-3dunet/blob/6e5a24b6438f8c631289c10638a17dea14d42051/unet3d/losses.py#L75
        paper: https://arxiv.org/pdf/1707.03237.pdf
        tf code: https://github.com/NifTK/NiftyNet/blob/dev/niftynet/layer/loss_segmentation.py#L279
        """
        super(GDiceLossV2, self).__init__()

        self.apply_nonlin = apply_nonlin
        self.smooth = smooth

    def forward(self, net_output, gt):
        shp_x = net_output.shape # (batch size,class_num,x,y,z)
        shp_y = gt.shape # (batch size,1,x,y,z)
        net_output=net_output.view([shp_x[0],shp_x[1],1,shp_x[2],shp_x[3]])
        gt=gt.view([shp_y[0],shp_y[1],1,shp_y[2],shp_y[3]])
        # one hot code for gt
        with torch.no_grad():
            if len(shp_x) != len(shp_y):
                gt = gt.view((shp_y[0], 1, *shp_y[1:]))

        softmax_output = torch.softmax(net_output, dim=1)
        y_onehot = gt
        input = flatten(softmax_output)
        target = flatten(y_onehot)
        target = target.float()
        target_sum = target.sum(-1)
        class_weights = Variable(1. / (target_sum * target_sum).clamp(min=self.smooth), requires_grad=False)

        intersect = (input * target).sum(-1) * class_weights
        intersect = intersect.sum()

        denominator = ((input + target).sum(-1) * class_weights).sum()

        return 1. - 2. * intersect / denominator.clamp(min=self.smooth)

def PixelWiseCrossEntropyLoss( input, target, class_weights):
        
        #assert target.size() == weights.size()
        
        # normalize the input
        log_softmax = nn.LogSoftmax(dim=1)
        log_probabilities = log_softmax(input)

        

        weights=torch.ones(input.size()).float().to(input.device)


        # resize class_weights to be broadcastable into the weights
        class_weights = class_weights.view(1, -1, 1, 1)
        
        
        # multiply weights tensor by class weights
        weights = class_weights * weights

        # compute the losses
        result = -weights * target * log_probabilities
        #print((-1*log_probabilities* target ).max(),target.max())
        # average the losses
        return result.mean()

def calc_loss(pred1, target, metrics,criterion,criterion2,TR,NR):

    
    img_size_i=250
    img_size_j=250
    ws=torch.tensor([TR,NR,1]).to(device).float()
    loss1 = PixelWiseCrossEntropyLoss( pred1, target,ws)
    #pp=nn.Softmax(dim=1)(pred1)
    loss2=criterion(pred1,target)
    loss3=criterion2(pred1,target)
    loss=0.33*loss1+0.33*loss2+0.33*loss3
    metrics['loss1'] += loss.data.cpu().numpy() * target.size(0)
    pp=nn.Softmax(dim=1)(pred1)
    res1=pp.cpu().detach().numpy() # ؟
    for i in range(0,np.size(res1,0)):
      res=np.argmax(res1[i,:,:,:],0)
      out1=np.zeros([img_size_i,img_size_j])
      out1[res==0]=1
      out2=np.zeros([img_size_i,img_size_j])
      out2[res==1]=1
      out3=np.zeros([img_size_i,img_size_j])
      out3[res==2]=1
      tt1=target[i,0,:,:]
      tt1=tt1.cpu().detach().numpy()
      if np.sum(tt1)>0:
        intersection = (out1 * tt1).sum()
        dice1=((2. * intersection ) / (out1.sum() + tt1.sum() ))
        metrics['n1']+=1
        metrics['dice1']=(1-(1/metrics['n1']))*metrics['dice1']+((1/metrics['n1'])*dice1)#؟
      tt2=target[i,1,:,:]
      tt2=tt2.cpu().detach().numpy()
      if np.sum(tt2)>0:
        intersection = (out2 * tt2).sum()
        dice2=((2. * intersection ) / (out2.sum() + tt2.sum() ))
        metrics['n2']+=1
        metrics['dice2']=(1-(1/metrics['n2']))*metrics['dice2']+((1/metrics['n2'])*dice2)
      tt3=target[i,2,:,:]
      tt3=tt3.cpu().detach().numpy()
      if np.sum(tt3)>0:
        intersection = (out3 * tt3).sum()
        dice3=((2. * intersection ) / (out3.sum() + tt3.sum() ))
        metrics['n3']+=1
        metrics['dice3']=(1-(1/metrics['n3']))*metrics['dice3']+((1/metrics['n3'])*dice3)

    

  
    return loss

def train_model(model, optimizer, scheduler ,criterion,criterion2,TR,NR,fold_number,num_epochs=25):
    best_model_wts = copy.deepcopy(model.state_dict())
    best_loss = 1e10

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)
        
        since = time.time()

        # Each epoch has a training and validation phase
        for phase in [ 'train','val']:
            if phase == 'train':
                scheduler.step()
                for param_group in optimizer.param_groups:
                    print("LR", param_group['lr'])
                    
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            metrics = defaultdict(float)
            epoch_samples = 0
            epoch_cor=0
            for inputs, labels ,lbp,sift in dataloaders[phase]:
                inputs = Variable(inputs.to(device))
                labels = Variable(labels.to(device))   
                lbp = Variable(lbp.to(device))   
                sift = Variable(sift.to(device))             
         
                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    outputs1= model(inputs,sift,lbp)
                    loss = calc_loss(outputs1,labels, metrics,criterion,criterion2,TR,NR)
                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                epoch_samples += inputs.size(0)
                #pp,pred=torch.max(outputs2.data,1)
                #epoch_cor+=torch.sum(pred==panc_labels.data).item()

            #print_metrics(metrics, epoch_samples, phase)
            epoch_loss = metrics['loss1'] / epoch_samples
            print(phase + ': '+ str(metrics['dice1'] ) +','+ str(metrics['dice2'] )+','+str(metrics['dice3'] ))
            file= open(output_path+'/Att_loss_'+str(fold_number)+'_'+phase+'.txt',"a+") 
            file.write(str(epoch_loss)+'\n')
            file= open(output_path+'/Att_backDice_'+str(fold_number)+'_'+phase+'.txt',"a+") 
            file.write(str(metrics['dice3'])+'\n')
            file= open(output_path+'/Att_TmrDice_'+str(fold_number)+'_'+phase+'.txt',"a+") 
            file.write(str(metrics['dice1'])+'\n')
            file= open(output_path+'/Att_NorDice_'+str(fold_number)+'_'+phase+'.txt',"a+") 
            file.write(str(metrics['dice2'])+'\n')
            # deep copy the model
            if phase == 'val' and epoch_loss < best_loss:
                print("saving best model")
                best_loss = epoch_loss
                best_model_wts = copy.deepcopy(model.state_dict())
            if phase=='val':
               torch.save(model.state_dict(), output_path+'/models/param_epoch'+str(epoch)+'.pt')
               torch.save(model, output_path+'/models/epoch'+str(epoch)+'.pt')

        time_elapsed = time.time() - since
        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
            
    print('Best val loss: {:4f}'.format(best_loss))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model

class AttU_Net(nn.Module):
    def __init__(self,img_ch=1,output_ch=1,nf=6):
        super(AttU_Net,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)
        n_filter=nf
        self.Conv1 = conv_block(ch_in=img_ch,ch_out=n_filter)
        self.Conv2 = conv_block(ch_in=n_filter,ch_out=n_filter*2)
        self.Conv3 = conv_block(ch_in=n_filter*2,ch_out=n_filter*4)
        self.Conv4 = conv_block(ch_in=n_filter*4,ch_out=n_filter*8)
        self.Conv5 = conv_block(ch_in=n_filter*8,ch_out=n_filter*16)

        self.Up5 = up_conv(ch_in=n_filter*16,ch_out=n_filter*8)
        self.Att5 = Attention_block(F_g=n_filter*8,F_l=n_filter*8,F_int=n_filter*4)
        self.Up_conv5 = conv_block(ch_in=n_filter*16, ch_out=n_filter*8)

        self.Up4 = up_conv(ch_in=n_filter*8,ch_out=n_filter*4)
        self.m1 = nn.BatchNorm2d(n_filter*4+128)
        self.Att4 = Attention_block(F_g=n_filter*4+128,F_l=n_filter*4,F_int=n_filter*2)
        self.Up_conv4 = conv_block(ch_in=n_filter*8, ch_out=n_filter*4)
        
        self.Up3 = up_conv(ch_in=n_filter*4,ch_out=n_filter*2)
        self.Att3 = Attention_block(F_g=n_filter*2,F_l=n_filter*2,F_int=n_filter)
        self.Up_conv3 = conv_block(ch_in=n_filter*4, ch_out=n_filter*2)
        
        self.Up2 = up_conv(ch_in=n_filter*2,ch_out=n_filter)
        self.m2 = nn.BatchNorm2d(n_filter+5)
        self.Att2 = Attention_block(F_g=n_filter+5,F_l=n_filter,F_int=int(n_filter/2))
        self.Up_conv2 = conv_block(ch_in=n_filter*2, ch_out=n_filter)

        self.Conv_1x1 = nn.Conv2d(n_filter,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x,sift,lbp):
        # encoding path
        x1 = self.Conv1(x)

        x2 = self.Maxpool(x1)
        x2 = self.Conv2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.Conv3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.Conv4(x4)

        x5 = self.Maxpool(x4)
        
        x5 = self.Conv5(x5)

        # decoding + concat path
        d5 = self.Up5(x5,x4.shape[2],x4.shape[3])
        x4 = self.Att5(g=d5,x=x4)
        d5 = torch.cat((x4,d5),dim=1)        
        d5 = self.Up_conv5(d5)
        
        d4 = self.Up4(d5,x3.shape[2],x3.shape[3])
        tmp1=self.m1(torch.cat((sift,d4),dim=1))
        x3 = self.Att4(g=tmp1,x=x3)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_conv4(d4)

        d3 = self.Up3(d4,x2.shape[2],x2.shape[3])
        x2 = self.Att3(g=d3,x=x2)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_conv3(d3)

        d2 = self.Up2(d3,x1.shape[2],x1.shape[3])
        tmp2=self.m2(torch.cat((lbp,d2),dim=1))
        x1 = self.Att2(g=tmp2,x=x1)
        #x1 = self.Att2(g=d2,x=x1)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_conv2(d2)

        d1 = self.Conv_1x1(d2)

        return d1

class PANC(Dataset):
    def __init__(self,data, transform=None):
        self.data_path='/hdd2/zahra/Data_MSD_138/lbp3d_r5_Data2d'
        self.data_path_img='/hdd2/zahra/Data_MSD_138/img_Data2d'
        self.label_path='/hdd2/zahra/Data_MSD_138/label2d'
        self.data_path_h='/hdd2/zahra/Data_MSD_138/new_data/hospital_data/lbp3d_r5_Data2d'
        self.data_path_img_h='/hdd2/zahra/Data_MSD_138/new_data/hospital_data/img_Data2d'
        self.label_path_h='/hdd2/zahra/Data_MSD_138/new_data/hospital_data/label2d'    
        self.transform = transform
        self.data=data
        self.img_size_i=250
        self.img_size_j=250
        self.key_points=[]
        for i in range(0,self.img_size_i-4,4):
          for j in range(0,self.img_size_j-4,4):
            self.key_points.append(cv2.KeyPoint(j,i,3))
        self.sift=sift = cv2.xfeatures2d.SIFT_create()
        self.dic={'pancreas407':'pancreas001','pancreas408':'pancreas002','pancreas409':'pancreas003','pancreas410':'pancreas005','pancreas411':'pancreas007','pancreas412':'pancreas011','pancreas413':'pancreas012','pancreas414':'pancreas016','pancreas415':'pancreas017','pancreas416':'pancreas018','pancreas417':'pancreas019','pancreas418':'pancreas020','pancreas400':'pancreas006','pancreas401':'pancreas014','pancreas402':'pancreas015','pancreas403':'pancreas021','pancreas404':'pancreas022','pancreas405':'pancreas023','pancreas406':'pancreas024'}
        

    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx): 
        ID=self.data[idx] 
        if ID[0:3]=='aug':
          if int(ID.split('_')[1][8])<4:
             image_path2=self.data_path_img+'/'+ID.split('_')[1]+'_'+ ID.split('_')[2] +'.npz'
             mask_path=self.label_path+'/'+ID.split('_')[1]+'_'+ ID.split('_')[2] +'.npz' 
             image_path=self.data_path+'/'+ID.split('_')[1]+'_'+ ID.split('_')[2] +'.npz'
          else:
             image_path2=self.data_path_img_h+'/'+self.dic[ID.split('_')[1]]+'_'+ ID.split('_')[2] +'.npz'
             mask_path=self.label_path_h+'/'+self.dic[ID.split('_')[1]]+'_'+ ID.split('_')[2] +'.npz' 
             image_path=self.data_path_h+'/'+self.dic[ID.split('_')[1]]+'_'+ ID.split('_')[2] +'.npz'

          image2= np.load(image_path2)
          image2=image2['arr_0']
          
          lbp3d= np.load(image_path)
          lbp3d=lbp3d['arr_0']

          mask =  np.load(mask_path)
          mask=mask['arr_0']
          i_start=random.randint(145-20,145+20)
          j_start=random.randint(167-20,167+20)
        else:
          if int(ID.split('_')[0][8])<4:
              image_path2=self.data_path_img+'/'+ID.split('_')[0]+'_'+ ID.split('_')[1] +'.npz'
              image_path=self.data_path+'/'+ID.split('_')[0]+'_'+ ID.split('_')[1] +'.npz'
              mask_path=self.label_path+'/'+ID.split('_')[0]+'_'+ ID.split('_')[1] +'.npz' 
          else:
              image_path2=self.data_path_img_h+'/'+self.dic[ID.split('_')[0]]+'_'+ ID.split('_')[1] +'.npz'
              image_path=self.data_path_h+'/'+self.dic[ID.split('_')[0]]+'_'+ ID.split('_')[1] +'.npz'
              mask_path=self.label_path_h+'/'+self.dic[ID.split('_')[0]]+'_'+ ID.split('_')[1] +'.npz' 
          image2= np.load(image_path2)
          image2=image2['arr_0']

          lbp3d= np.load(image_path)
          lbp3d=lbp3d['arr_0']

          mask =  np.load(mask_path)
          mask=mask['arr_0']
          i_start=145
          j_start=167
        
        
        mask1=mask[1,i_start:i_start+self.img_size_i,j_start:j_start+self.img_size_j]
        mask2=mask[2,i_start:i_start+self.img_size_i,j_start:j_start+self.img_size_j]
        mask1[mask1>0]=255
        mask2[mask2>0]=255
        mask3=np.zeros([self.img_size_i,self.img_size_j])+255
        mask3=mask3-mask1
        mask3=mask3-mask2
        mask3[mask3<0]=0
        #new added
        lower_bound = np.percentile(image2, 0.5)
        upper_bound = np.percentile(image2, 99.5)
        image2=np.clip(image2, lower_bound, upper_bound)
        img2=image2[i_start:i_start+self.img_size_i,j_start:j_start+self.img_size_j]
        #lbp3d
        lbp3d_0=lbp3d[0,i_start:i_start+self.img_size_i,j_start:j_start+self.img_size_j]
        lbp3d_1=lbp3d[1,i_start:i_start+self.img_size_i,j_start:j_start+self.img_size_j]
        lbp3d_2=lbp3d[2,i_start:i_start+self.img_size_i,j_start:j_start+self.img_size_j]
        lbp3d_3=lbp3d[3,i_start:i_start+self.img_size_i,j_start:j_start+self.img_size_j]

        
        #print(np.sum(mask1)/255,np.sum(mask2)/255,np.sum(mask3)/255,224*224,(np.sum(mask1)+np.sum(mask2)+np.sum(mask3))/255)

        if ID[0:3]=='aug':
          
          rr=np.random.randint(4)
          if rr!=3:

            img2=cv2.flip(img2, rr-1)
            mask1=cv2.flip(mask1, rr-1)
            mask2=cv2.flip(mask2, rr-1)
            mask3=cv2.flip(mask3, rr-1)
            lbp3d_0=cv2.flip(lbp3d_0, rr-1)
            lbp3d_1=cv2.flip(lbp3d_1, rr-1)
            lbp3d_2=cv2.flip(lbp3d_2, rr-1)
            lbp3d_3=cv2.flip(lbp3d_3, rr-1)
          rr=np.random.randint(4)
          if rr!=0:
            img2=np.rot90(img2, rr)
            mask1=np.rot90(mask1, rr)
            mask2=np.rot90(mask2, rr)
            mask3=np.rot90(mask3, rr)
            lbp3d_0=np.rot90(lbp3d_0, rr)
            lbp3d_1=np.rot90(lbp3d_1, rr)
            lbp3d_2=np.rot90(lbp3d_2, rr)
            lbp3d_3=np.rot90(lbp3d_3, rr)

 
      


        imgs=torch.rand(1,self.img_size_i,self.img_size_j)
        lbp_imgs=torch.rand(5,self.img_size_i,self.img_size_j)
        lbls=torch.rand(3,self.img_size_i,self.img_size_j)
        sift_img=torch.rand(128,int(self.img_size_i/4),int(self.img_size_j/4))
        ## should convert to uint8 for computing sift
        ### converting process:
        img_unit8=img2.copy()
        img_unit8=((img_unit8-np.min(img_unit8))/(np.max(img_unit8)-np.min(img_unit8)))*255
        img_unit8=img_unit8.astype(np.uint8)
        ###
        kp,des = self.sift.compute(img_unit8.copy(),self.key_points)
        des=np.transpose(des)
        des=des.reshape([128,int(self.img_size_i/4),int(self.img_size_j/4)])
        sift_img[:,:,:]=torch.tensor(des)
        radius = 3
        n_points = 10
        lbp2d = local_binary_pattern(img2.copy(), n_points, radius)
        lbp_imgs[0,:,:]=torch.tensor(lbp2d)
        lbp_imgs[1,:,:]=torch.tensor(lbp3d_0.copy())
        lbp_imgs[2,:,:]=torch.tensor(lbp3d_1.copy())
        lbp_imgs[3,:,:]=torch.tensor(lbp3d_2.copy())
        lbp_imgs[4,:,:]=torch.tensor(lbp3d_3.copy())

        imgs[0,:,:]=torch.tensor(img2.copy())

        lbls[0,:,:]=torch.tensor(mask1.copy()/255)
        lbls[1,:,:]=torch.tensor(mask2.copy()/255)
        lbls[2,:,:]=torch.tensor(mask3.copy()/255)
        return [imgs, lbls,lbp_imgs,sift_img]

kf=4
print('k is:',kf)
tmr_thresh=500
print('Tumor threshold is:',tmr_thresh)
n_aug_base=4
print('Base number of augmentation is:',n_aug_base)
filter_number=64
print('The Number of Filters is:',filter_number)
epoch_numbers=31
print('The Number of Epochs is:',epoch_numbers)

normal_ratio=np.load('NR.npy')
tumor_ratio=np.load('TR.npy')
output_path="Feature"
if os.path.exists(output_path)==False:
	os.mkdir(output_path)
if os.path.exists(output_path+'/models')==False:
	os.mkdir(output_path+'/models')

for i in range(1,2):
  print('*********************************************')
  print('fold',i)
  trans = transforms.Compose([
      transforms.ToTensor(),
    
  ])
  json_path='json_total.json'
 

  with open(json_path) as f:
      d = json.load(f)
  train_set = PANC(d['train'], transform=trans)
  val_set = PANC(d['val'], transform=trans)

  image_datasets = {
      'train': train_set, 'val': val_set
  }

  batch_size = 8

  dataloaders = {
      'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=5),
      'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=5)
  }

  dataset_sizes = {
      x: len(image_datasets[x]) for x in image_datasets.keys()
  }

  print(dataset_sizes)
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  print(device)
  model=AttU_Net(1,3,filter_number).to(device)
  # Observe that all parameters are being optimized
  optimizer_ft = optim.Adam(model.parameters(), lr=1e-5)
  #optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)
  criterion=GDiceLossV2()
  #criterion = TverskyLoss()

  #criterion2=BoundaryLoss()
  criterion2 = HausdorffDTLoss()

  model = train_model(model, optimizer_ft, exp_lr_scheduler,criterion,criterion2,tumor_ratio[i,0],normal_ratio[i,0],i, num_epochs=epoch_numbers)

